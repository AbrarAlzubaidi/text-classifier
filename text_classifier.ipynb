{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing and get files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Shape the data in an acceptable shape by Regression Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Train/classify the data using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib\n",
    "\n",
    "filePathDec = {'yelp':'data/yelp_labelled.txt',\n",
    "                'amazon':'data/amazon_cells_labelled.txt', \n",
    "                'imdb':'data/imdb_labelled.txt'\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make all data in files as list and change the extention as a csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                              sentence  label source\n",
       " 0                             Wow... Loved this place.      1   yelp\n",
       " 1                                   Crust is not good.      0   yelp\n",
       " 2            Not tasty and the texture was just nasty.      0   yelp\n",
       " 3    Stopped by during the late May bank holiday of...      1   yelp\n",
       " 4    The selection on the menu was great and so wer...      1   yelp\n",
       " ..                                                 ...    ...    ...\n",
       " 995  I think food should have flavor and texture an...      0   yelp\n",
       " 996                           Appetite instantly gone.      0   yelp\n",
       " 997  Overall I was not impressed and would not go b...      0   yelp\n",
       " 998  The whole experience was underwhelming, and I ...      0   yelp\n",
       " 999  Then, as if I hadn't wasted enough of my life ...      0   yelp\n",
       " \n",
       " [1000 rows x 3 columns],\n",
       "                                               sentence  label  source\n",
       " 0    So there is no way for me to plug it in here i...      0  amazon\n",
       " 1                          Good case, Excellent value.      1  amazon\n",
       " 2                               Great for the jawbone.      1  amazon\n",
       " 3    Tied to charger for conversations lasting more...      0  amazon\n",
       " 4                                    The mic is great.      1  amazon\n",
       " ..                                                 ...    ...     ...\n",
       " 995  The screen does get smudged easily because it ...      0  amazon\n",
       " 996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
       " 997                       Item Does Not Match Picture.      0  amazon\n",
       " 998  The only thing that disappoint me is the infra...      0  amazon\n",
       " 999  You can not answer calls with the unit, never ...      0  amazon\n",
       " \n",
       " [1000 rows x 3 columns],\n",
       "                                               sentence  label source\n",
       " 0    A very, very, very slow-moving, aimless movie ...      0   imdb\n",
       " 1    Not sure who was more lost - the flat characte...      0   imdb\n",
       " 2    Attempting artiness with black & white and cle...      0   imdb\n",
       " 3         Very little music or anything to speak of.        0   imdb\n",
       " 4    The best scene in the movie was when Gerardo i...      1   imdb\n",
       " ..                                                 ...    ...    ...\n",
       " 743  I just got bored watching Jessice Lange take h...      0   imdb\n",
       " 744  Unfortunately, any virtue in this film's produ...      0   imdb\n",
       " 745                   In a word, it is embarrassing.        0   imdb\n",
       " 746                               Exceptionally bad!        0   imdb\n",
       " 747  All in all its an insult to one's intelligence...      0   imdb\n",
       " \n",
       " [748 rows x 3 columns]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list=[]\n",
    "for source, filepath in filePathDec.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] =source\n",
    "    df_list.append(df)\n",
    "\n",
    "df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenating data as csv review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label source\n",
       "0                             Wow... Loved this place.      1   yelp\n",
       "1                                   Crust is not good.      0   yelp\n",
       "2            Not tasty and the texture was just nasty.      0   yelp\n",
       "3    Stopped by during the late May bank holiday of...      1   yelp\n",
       "4    The selection on the menu was great and so wer...      1   yelp\n",
       "..                                                 ...    ...    ...\n",
       "743  I just got bored watching Jessice Lange take h...      0   imdb\n",
       "744  Unfortunately, any virtue in this film's produ...      0   imdb\n",
       "745                   In a word, it is embarrassing.        0   imdb\n",
       "746                               Exceptionally bad!        0   imdb\n",
       "747  All in all its an insult to one's intelligence...      0   imdb\n",
       "\n",
       "[2748 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(df_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divid data for each file into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yelp data\n",
    "df_yelb = df[df['source'] == 'yelp']\n",
    "sentence_values_yelp = df_yelb['sentence'].values # x-axis\n",
    "label_values_yelp = df_yelb['label'].values # y-axis\n",
    "x_train_yelp, x_test_yelp, y_train_yelp, y_test_yelp = train_test_split(sentence_values_yelp, label_values_yelp, test_size=0.2, random_state=1)\n",
    "\n",
    "# for amazon data\n",
    "df_amazon = df[df['source'] == 'amazon']\n",
    "sentence_values_amazon = df_amazon['sentence'].values # x-axis\n",
    "label_values_amazon = df_amazon['label'].values # y-axis\n",
    "x_train_amazon, x_test_amazon, y_train_amazon, y_test_amazon = train_test_split(sentence_values_amazon, label_values_amazon, test_size=0.2, random_state=2)\n",
    "\n",
    "# for imdb data\n",
    "df_imdb = df[df['source'] == 'imdb']\n",
    "sentence_values_imdb = df_imdb['sentence'].values # x-axis\n",
    "label_values_imdb = df_imdb['label'].values # y-axis\n",
    "x_train_imdb, x_test_imdb, y_train_imdb, y_test_imdb = train_test_split(sentence_values_imdb, label_values_imdb, test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train data and transform it as series of 1 and 0 of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yelp data\n",
    "vectorizer_yelp = CountVectorizer()\n",
    "vectorizer_yelp.fit(x_train_yelp)\n",
    "\n",
    "transformed_x_train_yelp = vectorizer_yelp.transform(x_train_yelp)\n",
    "transformed_x_test_yelp = vectorizer_yelp.transform(x_test_yelp)\n",
    "\n",
    "# for amazon data\n",
    "vectorizer_amazon = CountVectorizer()\n",
    "vectorizer_amazon.fit(x_train_amazon)\n",
    "\n",
    "transformed_x_train_amazon = vectorizer_amazon.transform(x_train_amazon)\n",
    "transformed_x_test_amazon = vectorizer_amazon.transform(x_test_amazon)\n",
    "\n",
    "# for imdb data\n",
    "vectorizer_imdb = CountVectorizer()\n",
    "vectorizer_imdb.fit(x_train_imdb)\n",
    "\n",
    "transformed_x_train_imdb = vectorizer_imdb.transform(x_train_imdb)\n",
    "transformed_x_test_imdb = vectorizer_imdb.transform(x_test_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a model and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for yelp=  0.84\n",
      "score for amazon=  0.81\n",
      "score for imdb=  0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "# for yelp data\n",
    "classifier_yelp = LogisticRegression()\n",
    "classifier_yelp.fit(transformed_x_train_yelp, y_train_yelp)\n",
    "score_yelp = classifier_yelp.score(transformed_x_test_yelp, y_test_yelp)\n",
    "score_yelp\n",
    "\n",
    "# for amazon data\n",
    "classifier_amazon = LogisticRegression()\n",
    "classifier_amazon.fit(transformed_x_train_amazon, y_train_amazon)\n",
    "score_amazon = classifier_amazon.score(transformed_x_test_amazon, y_test_amazon)\n",
    "score_amazon\n",
    "\n",
    "# for imdb data\n",
    "classifier_imdb = LogisticRegression()\n",
    "classifier_imdb.fit(transformed_x_train_imdb, y_train_imdb)\n",
    "score_imdb = classifier_imdb.score(transformed_x_test_imdb, y_test_imdb)\n",
    "print('score for yelp= ', score_yelp)\n",
    "print('score for amazon= ', score_amazon)\n",
    "print('score for imdb= ', score_imdb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try to do keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4878/2720840844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/401-python/text-classifier/.venv/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a246e2ce62011b841eef7cece177295ce465184217a230aed2802b1c5613cf13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
